else if(x[i]<(0.5/n)){
sdown=n/(n-1)*(mean(s)-min(s)) #n vĂ©gtelen, ez konvergĂˇl egyhez
f0=mean(s)-sdown
plus=x[i]*n #kisebb, mint 0.5. ha nulla, akkor max sĂşlyt kap f0, ha 0,5 akkor max sĂşlyt kap a minimum
rg[i]=f0*(1-plus)+plus*min(s)
}
else{
sup=n/(n-1)*(max(s)-mean(s))
f1=mean(s)+sup
minus=-(1-x[i])*n
rg[i]=f1*(1-minus)+minus*max(s)
}}
rg
} #generates random number based on a given sample
r=rgen(5000,r0) #5000 numbers based on the distribution of R
m=c(r,r0)
plot(ecdf(m))
#c point
#
fi=runif(5000,0,2*pi)
x=r*cos(fi)
y=r*sin(fi)
plot(ecdf(x)) #based on the plot it is standard normal indeed
hist(x)
plot(density(x))
plot(ecdf(y)) #based on the plot it is standard normal indeed
#binomial tree generator
generate.tree=function(u,d,t){
n=t+1
tree=matrix(0,n,n)
for (i in 1:n){
for (j in 1:i){
tree[j,i]=u^(i-j)*d^(j-1)
}
}
tree
}
#value of option given the tree
value.tree=function(tree,rf,K){
#rf=0.01 #has to be commented
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]
}
#generate, and also value a tree
value.gen.tree=function(S,u,d,t,rf,K){
n=t+1
tree=matrix(0,n,n)
for (i in 1:n){
for (j in 1:i){
tree[j,i]=S*u^(i-j)*d^(j-1)
}
}
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
#speeding it up
value2.gen.tree=function(S,u,d,timeframe,rf,K){
n=timeframe+1
tree=matrix(0,n,n)
for (i in 1:n){
for (j in 1:i){
tree[j,i]=S*u^(i-j)*d^(j-1)
}
}
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
#binomial gecik
k=length(fwd)
binco=matrix(0,1,k)
for (i in 1:k){
binco[i]=ov^(k-i)*(1/(1+rf)-ov)^(i-1)*choose((k-1),(i-1))
}
a=sum(binco*fwd)
a*S
}
value.tree(generate.tree(2, .5, 3), 0, 10)
value.tree(generate.tree(2, .5, 3), 0, 100)
generate.tree(2, .5, 3)
value.gen.tree(100, 2, .5, 3, 0.01, 101)
value.gen.tree(100, 2, .5, 5, 0.01, 101)
value2.gen.tree(100, 2, .5, 4, 0, 100)
value.gen.tree(100, 2, .5, 3, 0.01, 100)
value.gen.tree(100, 2, .5, 3, 0, 100)
#binomial tree generator
generate.tree <- function(u = 2, d = .5,t){
tree <- matrix(0, t + 1, t + 1)
for (i in 1:(t + 1)){
for (j in 1:i){
tree[ j , i ] = u ^ ( i - j ) * d ^ ( j - 1 )
}
}
tree
}
#value of option given the tree
value.tree=function(tree,rf = 0,K){
Kn <- K/tree[1,1]#normalizing
t <- tree/tree[1,1]
r <- nrow(t)
u <- (t[1,r])^(1/(r-1))
d <- (t[r,r])^(1/(r-1))
#{1,0} option value
ov <- (1-(d/(1+rf)))/(u-d)
lastcol <- t[,r]
fwd <- lastcol-Kn
fwd[ fwd < 0 ] = 0
k <- length(fwd)
rollback <- matrix(0,k,k)
rollback[,1] = fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col] = rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[ 1, k]
}
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree( u, d, t)
value.tree( tree, rf, K) * S
}
value.gen.tree(t = 5, K = 100)
value.gen.tree(S = 100,t = 5, K = 100)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree( u, d, t)
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
value.gen.tree(S = 100,t = 5, K = 100)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K){
tree <- generate.tree( u, d, t)
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
value.gen.tree(S = 100,t = 5, K = 100)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
n=t+1
tree=matrix(0,n,n)
for (i in 1:n){
for (j in 1:i){
tree[j,i]=S*u^(i-j)*d^(j-1)
}
}
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
value.gen.tree(S = 100,t = 5, K = 100)
value.gen.tree(S = 100,t = 3, K = 100)
tree <- matrix(0, t + 1, t + 1)
t = 4
t = 4
tree <- matrix(0, t + 1, t + 1)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree( u, d, t)
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
value.gen.tree(S = 100,t = 3, K = 100)
value.gen.tree(S = 100,u = 2, d = .5, t = 3, K = 100)
t = 5
n=t+1
tree=matrix(0,n,n)
for (i in 1:n){
for (j in 1:i){
tree[j,i]=S*u^(i-j)*d^(j-1)
}
}
#binomial tree generator
generate.tree <- function(u ,d ,t, S){
tree <- matrix(0, t + 1, t + 1)
for (i in 1:(t + 1)){
for (j in 1:i){
tree[ j , i ] = S* u ^ ( i - j ) * d ^ ( j - 1 )
}
}
tree
}
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
generate.tree(u, d, t, S)
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
value.gen.tree(S = 100,u = 2, d = .5, t = 3, K = 100)
generate.tree(u, d, t, S)
S = 100
generate.tree(u, d, t, S)
u = 2
d = .5
generate.tree(u, d, t, S)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree(u, d, t, S)
Kn=K/tree[1,1]#normalizing
t=tree/tree[1,1]
r=nrow(t)
u=(t[1,r])^(1/(r-1))
d=(t[r,r])^(1/(r-1))
#{1,0} option value
ov=(1-(d/(1+rf)))/(u-d)
lastcol=t[,r]
fwd=lastcol-Kn
fwd[fwd<0]=0
k=length(fwd)
rollback=matrix(0,k,k)
rollback[,1]=fwd
for (col in 2:k){
for (row in 1:(k+1-col)){
rollback[row,col]=rollback[(row+1),(col-1)]/(1+rf)+(rollback[row,(col-1)]-rollback[(row+1),(col-1)])*ov
}
}
rollback[1,k]*S
}
value.gen.tree(S = 100,u = 2, d = .5, t = 3, K = 100)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree(u, d, t, S)
value.tree(tree, rf, K) * S
}
value.gen.tree(t  = 3)
value.gen.tree(S = 2000, t  = 3)
value.gen.tree(S = 2000, t  = 3, K = 2000)
value.gen.tree(t = 5)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree(u, d, t, S)
print(tree)
value.tree(tree, rf, K) * S
}
value.gen.tree(t = 5)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree(u, d, t, S)
print(tree)
print( paste( "the call option's price for this tree is", value.tree(tree, rf, K) * S))
}
value.gen.tree(t = 5)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree(u, d, t, S)
print(tree)
print( paste( "the call option's price for this tree is", round( value.tree(tree, rf, K) * S), 3))
}
value.gen.tree(t = 5)
#generate, and also value a tree
value.gen.tree=function(S = 100, u = 2, d = .5, t, rf = 0, K = S){
tree <- generate.tree(u, d, t, S)
print(tree)
print( paste( "the call option's price for this tree is", round( value.tree(tree, rf, K) * S, 3)))
}
value.gen.tree(t = 5)
#lehetne szep reszvenyarfolyamokat is letolteni es ratenni az armat, de az ellentmondana a hatekonysagnak
RawData <-  ax.xts( ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) ))  #TÉNYLEG ARIMA(2,1,3)!!!!!!!!!!!!!!!!!!!!!
#lehetne szep reszvenyarfolyamokat is letolteni es ratenni az armat, de az ellentmondana a hatekonysagnak
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) )  #TÉNYLEG ARIMA(2,1,3)!!!!!!!!!!!!!!!!!!!!!
head(RawData)
install.packages("forecast")
tsdisplay( RawData) #gyanus hogy nem stacioner
forecast::tsdisplay( RawData) #gyanus hogy nem stacioner
tseries::adf.test( RawData) #nem stacioner, mert a nullhipoteyist nem vetjuk el. ay pedig az hogy egyseggyok van
tseries::kpss.test( RawData)
install.packages("urca")
install.packages("urca")
urca::summary( urca::ur.df( RawData))
forecast::Acf( diff( RawData))
tsdisplay( diff(RawData)) #gyanus hogy stacioner, de tesytelni kell
forecast::tsdisplay( diff(RawData)) #gyanus hogy stacioner, de tesytelni kell
tseries::kpss.test( diff( RawData)) #stacioner, mert a kpss nullhipotezise az hogz nincs egységgyök, és ezt nem vethetjük el
#forecast can do it with a one-line for finding the number of integration
forecast::ndiffs( RawData)
#forecast can do it with a one-line for finding the number of integration
d <- forecast::ndiffs( RawData )
predgrid <- expand.grid( p = 0:5, q = 0:5 )
predgrid <- expand.grid( p = 0:5, q = 0:5 )
predgrid$AIC <- apply(predgrid, 1, function( x ) AIC(forecast::Arima( RawData, order = c(x[1], d, x[2] ) ) ) ) #konv hibauzenet a sima arimanak. ezert hasznaljak a forecast packaget. am semmi jelentosege ayoknak a warningoknak
predgrid
library(lattice)
lattice::wireframe( AIC~ p + q, data = predgrid)
lattice::levelplot( AIC~ p + q, data = predgrid)
# p,q = 2,3 or 3,4 could be the case
which.min(predgrid$AIC)
# p,q = 2,3 or 3,4 could be the case
which.min(predgrid$AIC)["p"]
# p,q = 2,3 or 3,4 could be the case
which.min(predgrid$AIC)[["p"]]
# p,q = 2,3 or 3,4 could be the case
which.min(predgrid$AIC)[[p]]
i <- which.min(predgrid$AIC)
fit <- arima( RawData, order = c( predgrid[i,1], d, predgrid[i,2]) ) #a kozepso argumentumot tudjuk
summary(fit)
predgrid[i,1]
predgrid[i,2]
tsdiag( fit) #akkor jo a modell, ha a residumokban nincsen korrelacio, ez nem olyan jo modell eszerint. a ljung box proba eredmenye a p ertek a lag fuggvenyeben. a nullhi
qqnorm( resid( fit) ) #legjobb grafikus tesztelese a normalitasnak. egyenesnek kell lennie
qqline( resid( fit) )
forecast(fit)
forecast::forecast(fit)
plot( forecast( fit), xlim = c( 1000, 1025)) #legyezo abra
plot( forecast::forecast( fit), xlim = c( 1000, 1025)) #legyezo abra
#hold-out sample validation
fitTrain <- arima( RawData[ 1:1000 ], order = c( predgrid[i,1], d, predgrid[i,2]) ) #a kozepso argumentumot tudjuk
#hold-out sample validation
fitTrain <- forecast::Arima( RawData[ 1:1000 ], order = c( predgrid[i,1], d, predgrid[i,2]) ) #a kozepso argumentumot tudjuk
forecast::accuracy(forecast(fitTrain), RawData[ 1001:1010])
forecast::accuracy(forecast::forecast(fitTrain), RawData[ 1001:1010])
#honnan tudjuk p-t es q-t az armahoz?
AIC(fit)
forecast::auto.arima( RawData, trace = T) #ez megcsinal mindent amit mi eddig csinaltunk
#generate a dataset that follows a (2,1,3) ARIMA Process
set.seed(100120)
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) )
#generate a dataset that follows a (2,1,3) ARIMA Process
set.seed(100120)
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) )
forecast::tsdisplay( RawData )
tseries::adf.test( RawData )
tseries::kpss.test( RawData )
urca::summary( urca::ur.df( RawData ) )
forecast::Acf( diff( RawData ) )
forecast::tsdisplay( diff(RawData ) )
tseries::kpss.test( diff( RawData ) )
#forecast can do it with a one-line for finding the number of integration
d <- forecast::ndiffs( RawData )
#finding the value of p,q with grid search
predgrid <- expand.grid( p = 0:5, q = 0:5 )
predgrid$AIC <- apply(predgrid, 1, function( x ) AIC(forecast::Arima( RawData, order = c(x[1], d, x[2] ) ) ) )
lattice::wireframe( AIC~ p + q, data = predgrid)
lattice::levelplot( AIC~ p + q, data = predgrid)
i <- which.min(predgrid$AIC)
#lets fit an arima
fit <- arima( RawData, order = c( predgrid[i,1], d, predgrid[i,2]) )
summary(fit)
tsdiag( fit )
qqnorm( resid( fit ) )
qqline( resid( fit ) )
forecast::forecast(fit)
plot( forecast::forecast( fit), xlim = c( 1000, 1025)) #legyezo abra
plot( forecast::forecast( fit), xlim = c( 1000, 1025), ylim = c( -20, 50 ) )  #legyezo abra
forecast::forecast(fit)
plot( forecast::forecast( fit), xlim = c( 1000, 1025), ylim = c( -20, 50 ) )  #legyezo abra
plot( forecast::forecast( fit), xlim = c( 1000, 1025), ylim = c( -20, -50 ) )  #legyezo abra
plot( forecast::forecast( fit), xlim = c( 1000, 1025), ylim = c( -60, -30 ) )  #legyezo abra
#hold-out sample validation
fitTrain <- forecast::Arima( RawData[ 1:1000 ], order = c( predgrid[i,1], d, predgrid[i,2]) ) #a kozepso argumentumot tudjuk
forecast::accuracy(forecast::forecast(fitTrain), RawData[ 1001:1010])
forecast::auto.arima( RawData, trace = T) #ez megcsinal mindent amit mi eddig csinaltunk
#generate a dataset that follows a (2,1,3) ARIMA Process
set.seed(100120)
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) )
#####a forecast one-line for everything we covered:
forecast::auto.arima( RawData, trace = T)
#generate a dataset that follows a (2,1,3) ARIMA Process
set.seed(100)
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) )
#####a forecast one-line for everything we covered:
forecast::auto.arima( RawData, trace = T)
#generate a dataset that follows a (2,1,3) ARIMA Process
set.seed(103030)
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -.5),
ma = c(-.2, .2, .5) ) ) ) )
#####a forecast one-line for everything we covered:
forecast::auto.arima( RawData, trace = T)
#generate a dataset that follows a (2,1,3) ARIMA Process
set.seed(12303024)
RawData <-  ts ( cumsum( arima.sim( n = 1010,
list( ar = c(0.7, -0.5),
ma = c(-0.2, 0.2, 0.5) ) ) ) )
#####a forecast one-line for everything we covered:
forecast::auto.arima( RawData, trace = T)
